---
title: 'Engy Fouda Assignment 6'
output: html_document
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```


Short example of code shown below illustrates reading this data from a local copy on your computer (assuming it has been copied into current working directory of your R session -- `getwd()` and `setwd()` commands are helpful to find out what is it currently and change it to desired location) and displaying summaries and pairs plot of five (out of almost 40) arbitrary chosen variables.  This is done for illustration purposes only -- the problems in the assignment expect use of all variables in this dataset.

```{r WHS}
whsAnnBdatNum <- read.table("C:/Users/Engy/Downloads/Harvard/7th semester ISA/R/Week 6/whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whsAnnBdatNum)
#pairs(whsAnnBdatNum)
```

In some way this dataset is somewhat similar to the `USArrests` dataset extensively used in ISLR labs and exercises -- it collects various continuous statistics characterizing human population across different territories.  It is several folds larger though -- instead of `r nrow(USArrests)` US states and `r ncol(USArrests)` attributes in `USArrests`, world health statistics (WHS) data characterizes `r nrow(whsAnnBdatNum)` WHO member states by `r ncol(whsAnnBdatNum)` variables.  Have fun!

```{r}
nrow(whsAnnBdatNum)
```
```{r}
ncol(whsAnnBdatNum)
```

The following problems are largely modeled after labs and exercises from Chapter 10 ISLR.  If anything presents a challenge, besides asking questions on piazza (that is always a good idea!), you are also encouraged to review corresponding lab sections in ISLR Chapter 10.

# Problem 1: Principal components analysis (PCA) (25 points)

## Sub-problem 1a: means and variances of WHS attributes (5 points)

Compare means and variances of the attributes in the world health statisics dataset. Function `apply` allows to apply desired function (e.g. `mean` or `var`) to each row or column in the table.  Do you see all `r ncol(whsAnnBdatNum)` attributes in the plot, or at least most of them?  (Remember that you can use `plot(inpX,inpY,log="xy")` to use log-scale on both horizontal and vertical axes.) 

```{r}
meanAttr=apply(whsAnnBdatNum , 2, mean)
meanAttr
```

I see all of them, they are 37 attributes

```{r}
varAttr=apply(whsAnnBdatNum , 2, var)
varAttr
```

-- plot of variance vs. mean is probably the best given number of attributes in the dataset.  

```{r}
plot(log(varAttr),log(meanAttr),type='o',col="red")
#plot(log(meanAttr),log(varAttr),type='o',col="red")
```
 
 What is the range of means and variances when calculated on untransformed data?  Which are the top two attributes with highest mean or variance?  

```{r}
range(varAttr)
range(meanAttr)
```


```{r}
which.max(varAttr)
```

```{r}
which.min(varAttr)
```

```{r}
which.max(meanAttr)
```

```{r}
which.min(meanAttr)
```

```{r}
n <- length(varAttr)
sort(varAttr,partial=n-1)[n-1]
```
This is the value of TOPTOP

```{r}
n <- length(meanAttr)
sort(meanAttr,partial=n-1)[n-1]
```
This is the value of TOPTOP as well  

What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

According to this lecture: https://www.youtube.com/watch?v=lFHISDj_4EQ

After briefly examining the data, notice that the variables have vastly different means and variances. PCA is about varinces not means. But the variances of individual variables will play a role. After all, we're looking for linear combination that the largest principal components, the linear combination that maximizes the variance. And if you've got a single variable that dominates with
respect to variance, like INTINTDS here, it'll pretty much eat up the first principle component.
And so you won't get anything meaningful out of the data. And these variances are largely due to the fact that there's different units. Each of the variables is measured in different units.
So all that's to say we going to standardize the variables before we do principal components.
We standardize them all to have unit variances. So prcomp will actually do that when sets the scale to true. 

## Sub-problem 1b: PCA on untransformed data (10 points)

Perform PCA on *untransformed* data in WHS dataset (remember, you can use R function `prcomp` for that).  
```{r}
#pr.out =prcomp (whsAnnBdatNum , scale =TRUE)
pr.out =prcomp (whsAnnBdatNum , scale =FALSE)
names(pr.out)
pr.out
```

Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`) 

```{r}
plot(pr.out)
```


plot of the two first principal components using `biplot`.  

```{r}
#biplot(pr.out,pc.biplot = T)
biplot(pr.out)
```

Which variables seem to predominantly drive the results of PCA when applied to untransformed data?

```{r}
#pr.out$center
```

```{r}
sort(pr.out$center,decreasing = TRUE)
```

INTINTDS, TOPTOP will predominate because they are the highest. The untransformed data has different units.     

```{r}
#pr.out$scale
```

Please note that in this case you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?  


The error message is: "zero-length arrow is of indeterminate angle and so skipped." this indicates that the confidence limits are of length 0, as there's no variability per condition.  



The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.  

```{r}

pr.out$rotation
dim(pr.out$x)
biplot(pr.out, scale=0)

#pr.out$rotation=-pr.out$rotation
#pr.out$x=-pr.out$x
#biplot(pr.out, scale=0)

```


What attributes have the largest (by their absolute value) loadings for the first and second principal component?  

```{r}
which.max(abs(pr.out$rotation[,1]))
which.max(abs(pr.out$rotation[,2]))
```

How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?

The max.and the second maxium value are the same as INITINTDS, TOPTOP.  



Calculate percentage of variance explained (PVE) by the first five principal components (PCs). 
```{r}
pr.out$sdev
pr.var=pr.out$sdev^2
pr.var
pve=pr.var/sum(pr.var)
pve
summary(pve)
summary(pr.out)
```


```{r}
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')

```

All the previous results are for the unscaled data, that is why they look horrible!
As Dr. Victor here in https://piazza.com/class/iw8s0jb4nto2hc?cid=594 
for =untransformed= data is to observe first hand that when measurements are made on so different scales the attribute(s) with the highest variance by far dominate the PCA results - they are not going to be pretty.

To have significant meanigful results, I'll try to scale and repeat the previous steps:

```{r}
pr.out1 =prcomp (whsAnnBdatNum , scale =TRUE)
#pr.out1$sdev
pr.var1=pr.out1$sdev^2
#pr.var1
pve1=pr.var1/sum(pr.var1)
pve1
# summary(pve1)
# summary(pr.out1)
plot(pve1, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve1), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')

```

Here, we can explain the variance as percentages:
The first principal component explains 39.7% of the variance in the data, the second principal component explains 6.3% of the variance, the third one explains 6.1%, the fourth is 5.7%, the fifth is 5.1%.  

As in page 409-410 in ISLR book: This is not a huge amount of the variance. However, looking at the scree plot, we see that while each of the first two principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. That is, there is an elbow in the plot after approximately the second principal component. This suggests that there may be little benefit to examining more than two or so principal components (though even examining two principal components may be difficult).

Lastly, perform PCA on *transposed* (but still *untransformed*) WHS dataset -- remember that in R `t(x)` returns transpose of `x`:

```{r}
#matrix(1:6,ncol=3)
#t(matrix(1:6,ncol=3))
pr.out =prcomp (t(whsAnnBdatNum), scale =FALSE)
pr.out
```

  

Present results of PCA on transposed world health statistics dataset in the form of scree and biplot, describe the results.

```{r}
plot(pr.out)
#biplot(pr.out,pc.biplot = T)
biplot(pr.out)
```

## Sub-problem 1c: PCA on scaled WHS data (10 points)

Perform PCA on scaled world health statistics data.  To do that you can either use as input to `prcomp` the output of`scale` as applied to the WHS data matrix or call `prcomp` with parameter `scale` set to `TRUE`.  Present results of PCA in the form of scree plot and plot of the first two principal components.  

```{r}
pca.scaled=prcomp(whsAnnBdatNum , scale =TRUE)
#plot(pca.scaled$x[,1:2])
#biplot(pca.scaled$x[,1:2])
plot(pca.scaled)
biplot(pca.scaled)
```


How do they compare to those generated on the results of PCA of *untransformed* data?  

The pr.out plot for the untransformed data has only one bar of variance higher than 1.5+e15 and the rest are flat, while in the scaled data, better representation. The first attirbute isstill dominating but by variance higher than 14 and the rest attributes have some varience representation of variance from 2 or less.  

What dataset attributes contribute the most (by absolute value) to the top two PCs?  What are the signs of those contributions?  How would you interpret that?

```{r}
pca.scaled$rotation
dim(pca.scaled$x)
biplot(pca.scaled, scale=1)

which.max(abs(pca.scaled$rotation[,1]))
which.max(abs(pca.scaled$rotation[,2]))
```

They are different from those generated from the untransformed data.  
Here they are LIFEXPB.F,HOMICIDE.  
Their signs of these contributers are negative. 




The output of `biplot` with almost 200 text labels on it is pretty busy and could be tough to read.  You can achieve better control when plotting PCA results if instead you plot the first two columns of the `x` attribute in the output of `prcomp` -- e.g. `plot(prcomp(USArrests,scale=T)$x[,1:2])`.  Use this to label a subset of countries on the plot -- you can use `text` function in R to add labels at specified positions on the plot -- please feel free to choose several countries of your preference and discuss the results.  Alternatively, indicate US, UK, China, India, Mexico, Australia, Israel, Italy, Ireland and Sweden and discuss the results.  

https://piazza.com/class/iw8s0jb4nto2hc?cid=525

```{r}

plot(pca.scaled$x[,1:2])

countries<-c("UnitedStatesofAmerica","UnitedKingdom","China","India","Mexico","Australia","Italy","Ireland","Sweden")

index.countries<-row.names(pca.scaled$x[,1:2]) %in% countries

points(pca.scaled$x[index.countries,1:2],col="red",pch=19)

text(pca.scaled$x[index.countries,1:2],labels = row.names(pca.scaled$x[index.countries,1:2]),pos=2,cex=.5)

pca.scaled$x[index.countries,1:2]
```

Where do the countries you have plotted fall in the graph?   

India is the highest, then China, then Mexico, then the United Kindom.  



Finally, perform PCA on *transposed* scaled WHS dataset -- present results in the form of scree plot and biplot and discuss these presentations.

```{r}
pca.scaled.transp =prcomp (t(whsAnnBdatNum), scale =TRUE)
pca.scaled.transp
plot(pca.scaled.transp)
biplot(pca.scaled.transp)
```


### For *extra 8 points*

Try the following:

* Instead of scaling (or prior to scaling) perform log-transform of the data before passing it on to `prcomp`.  Given that some of the attributes have zero values, you will have to decide how to handle those to avoid negative infinity upon log-transformation.  Usually, a small positive (often a fraction of smallest non-zero value for that attribute) value is added to all (or just zero) values of the attribute that contains zeroes.  Present and describe the results.

```{r}
pca.log =prcomp (log(whsAnnBdatNum+1))
pca.log
plot(pca.log)
biplot(pca.log)
```

The plots are different from the untransformed and the scaled. However teh log is closer in variance to teh scaled dataset than that of the untransformed. 

* Demonstrate equivalence of the results as obtained by `prcomp(x)` and `cmdscale(dist(x))` where `x` represents scaled WHS dataset.

```{r}
#cmd.scale.whs=cmdscale(dist(pca.scaled))
whs.scale = scale(whsAnnBdatNum)
cmd <- cmdscale(dist(whs.scale))
pr <- prcomp(whs.scale)
cor(pr$x[,1], cmd[,1])
cor(pr$x[,2], cmd[,2])

```

* Explore use of multidimensional scaling (MDS) tools available in library `MASS` such as `sammon` and `isoMDS`.  Present their results and discuss the differences between them and PCA output.  No, there was nothing on that in the lecture -- thus it is for extra points and to broaden your horizons.

```{r}
library(MASS)
d = dist(whs.scale)
whs.mds = isoMDS(d)

sammon(d)
#isoMDS(whs.scale)
```



# Problem 2: K-means clustering (15 points)

## Sub-problem 2a: k-means clusters of different size (5 points)

Using function `kmeans` perform K-means clustering on *explicitly scaled* (e.g. `kmeans(scale(x),2)`) world health statistics data for 2, 3 and 4 clusters.  Use `cluster` attribute in the output of `kmeans` to indicate cluster membership by color and/or shape of the corresponding symbols in the plot of the first two principal components generated independently on the same (scaled WHS) data.  E.g. `plot(prcomp(xyz)$x[,1:2],col=kmeans(xyz,4)$cluster)` where `xyz` is input data.  Describe the results.  Which countries are clustered together for each of these choices of $K$?


```{r}
kmeans(scale(whsAnnBdatNum),2)
kmeans(scale(whsAnnBdatNum),3)
kmeans(scale(whsAnnBdatNum),4)

plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),2)$cluster)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),3)$cluster)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4)$cluster)
```

## Sub-problem 2b: variability of k-means clustering (5 points)

By default, k-means clustering uses random set of centers as initial guesses of cluster centers.  Here we will explore variability of k-means cluster membership across several such initial random guesses.  To make such choices of random centers reproducible, we will use function `set.seed` to reset random number generator (RNG) used in R to make those initial guesses to known/controlled initial state.

Using the approach defined above, repeat k-means clustering with four clusters three times resetting RNG each time with `set.seed` using seeds of 1, 2 and 3 respectively.  Indicate cluster membership in each of these three trials on the plot of the first two principal components using color and/or shape as described above.  Two fields in the output of `kmeans` -- `tot.withinss` and `betweenss` -- characterize within and between clusters sum-of-squares.  Tighter clustering results are those which have smaller ratio of within to between sum-of-squares.  What are the resulting ratios of within to between sum-of-squares for each of these three k-means clustering results (with random seeds of 1, 2 and 3)?


```{r}
 set.seed (1)

kmeans(scale(whsAnnBdatNum),4)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4)$cluster)
ratio1=kmeans(scale(whsAnnBdatNum),4)$tot.withinss/kmeans(scale(whsAnnBdatNum),4)$betweenss



set.seed (2)
kmeans(scale(whsAnnBdatNum),4)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4)$cluster)
ratio2=kmeans(scale(whsAnnBdatNum),4)$tot.withinss/kmeans(scale(whsAnnBdatNum),4)$betweenss



set.seed (3)
kmeans(scale(whsAnnBdatNum),4)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4)$cluster)
ratio3=kmeans(scale(whsAnnBdatNum),4)$tot.withinss/kmeans(scale(whsAnnBdatNum),4)$betweenss

```




## Sub-problem 2c: effect of `nstarts` parameter (5 points)

Repeat the procedure implemented for the previous sub-problem (k-means with four clusters for RNG seeds of 1, 2 and 3) now using 100 as `nstart` parameter in the call to `kmeans`.  Represent results graphically as before.  How does cluster membership compare between those three runs now?  What is the ratio of within to between sum-of-squares in each of these three cases?  


```{r}

set.seed (1)

k1=kmeans(scale(whsAnnBdatNum),4,nstart =100)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4,nstart=100)$cluster)
rationstart1<-kmeans(scale(whsAnnBdatNum),4,nstart =100)$tot.withinss/kmeans(scale(whsAnnBdatNum),4,nstart = 100)$betweenss


set.seed (2)

k2=kmeans(scale(whsAnnBdatNum),4,nstart =100)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4,nstart=100)$cluster)
rationstart2<-kmeans(scale(whsAnnBdatNum),4,nstart =100)$tot.withinss/kmeans(scale(whsAnnBdatNum),4,nstart = 100)$betweenss


 set.seed (3)

k3=kmeans(scale(whsAnnBdatNum),4,nstart =100)
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2],col=kmeans(scale(whsAnnBdatNum),4,nstart=100)$cluster)
rationstart3<-kmeans(scale(whsAnnBdatNum),4,nstart =100)$tot.withinss/kmeans(scale(whsAnnBdatNum),4,nstart = 100)$betweenss

```

```{r}
ratio1
ratio2
ratio3
```


```{r}

rationstart1
rationstart2
rationstart3
```

What is the impact of using higher than 1 (default) value of `nstart`?  

Before using nstart, the lower seed value always gets the higher ratio.  
But after using the nstart as 100, the ratios are all equal.  

What is the ISLR recommendation on this offered in Ch. 10.5.1?  

The ISLR recomendations is as follows:  
"We strongly recommend always running K-means clustering with a large value of nstart, such as 20 or 50, since otherwise an undesirable local optimum may be obtained. When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the set.seed() function. This way, the initial cluster assignments in Step 1 can be replicated, and the K-means output will be fully reproducible."  



### For *extra 8 points*

Try the following:

* evaluate dependency between the stability of k-means clustering and the number of clusters and values of `nstarts`; to make this more quantitative consider using contingency table (i.e. `table`) to quantify concordance of two different clustering results (E.g. how many non-zero cells would be in the output of `table` for two perfectly concordant clustering assignments?)

#https://piazza.com/class/iw8s0jb4nto2hc?cid=530
#nstart values (1,2,5,10) for different values of K (2,5,10), 

```{r}
df = data.frame(cluster= numeric(0), nstart= numeric(0),ratio= double(0))

set.seed (100)

df = rbind(df, data.frame(cluster = 2, nstart = 1, ratio = kmeans(scale(whsAnnBdatNum),2,nstart =1)$tot.withinss/kmeans(scale(whsAnnBdatNum),2,nstart =1)$betweenss))
df = rbind(df, data.frame(cluster = 2, nstart = 2, ratio = kmeans(scale(whsAnnBdatNum),2,nstart =2)$tot.withinss/kmeans(scale(whsAnnBdatNum),2,nstart =2)$betweenss))
df = rbind(df, data.frame(cluster = 2, nstart = 5, ratio =kmeans(scale(whsAnnBdatNum),2,nstart =5)$tot.withinss/kmeans(scale(whsAnnBdatNum),2,nstart =5)$betweenss))
df = rbind(df, data.frame(cluster = 2, nstart = 10, ratio =kmeans(scale(whsAnnBdatNum),2,nstart =10)$tot.withinss/kmeans(scale(whsAnnBdatNum),2,nstart =10)$betweenss))

df = rbind(df, data.frame(cluster = 5, nstart = 1, ratio =kmeans(scale(whsAnnBdatNum),5,nstart =1)$tot.withinss/kmeans(scale(whsAnnBdatNum),5,nstart =1)$betweenss))
df = rbind(df, data.frame(cluster = 5, nstart = 2, ratio =kmeans(scale(whsAnnBdatNum),5,nstart =2)$tot.withinss/kmeans(scale(whsAnnBdatNum),5,nstart =2)$betweenss))
df = rbind(df, data.frame(cluster = 5, nstart = 5, ratio =kmeans(scale(whsAnnBdatNum),5,nstart =5)$tot.withinss/kmeans(scale(whsAnnBdatNum),5,nstart =5)$betweenss))
df = rbind(df, data.frame(cluster = 5, nstart = 10, ratio =kmeans(scale(whsAnnBdatNum),5,nstart =10)$tot.withinss/kmeans(scale(whsAnnBdatNum),5,nstart =10)$betweenss))


df = rbind(df, data.frame(cluster = 10, nstart = 1, ratio =kmeans(scale(whsAnnBdatNum),10,nstart =1)$tot.withinss/kmeans(scale(whsAnnBdatNum),10,nstart =1)$betweenss))
df = rbind(df, data.frame(cluster = 10, nstart = 2, ratio =kmeans(scale(whsAnnBdatNum),10,nstart =2)$tot.withinss/kmeans(scale(whsAnnBdatNum),10,nstart =2)$betweenss))
df = rbind(df, data.frame(cluster = 10, nstart = 5, ratio =kmeans(scale(whsAnnBdatNum),10,nstart =5)$tot.withinss/kmeans(scale(whsAnnBdatNum),10,nstart =5)$betweenss))
df = rbind(df, data.frame(cluster = 10, nstart = 10, ratio =kmeans(scale(whsAnnBdatNum),10,nstart =10)$tot.withinss/kmeans(scale(whsAnnBdatNum),10,nstart =10)$betweenss))


df = rbind(df, data.frame(cluster = 50, nstart = 1, ratio =kmeans(scale(whsAnnBdatNum),50,nstart =1)$tot.withinss/kmeans(scale(whsAnnBdatNum),50,nstart =1)$betweenss))
df = rbind(df, data.frame(cluster = 50, nstart = 2, ratio =kmeans(scale(whsAnnBdatNum),50,nstart =2)$tot.withinss/kmeans(scale(whsAnnBdatNum),50,nstart =2)$betweenss))
df = rbind(df, data.frame(cluster = 50, nstart = 5, ratio =kmeans(scale(whsAnnBdatNum),50,nstart =5)$tot.withinss/kmeans(scale(whsAnnBdatNum),50,nstart =5)$betweenss))
df = rbind(df, data.frame(cluster = 50, nstart = 10, ratio =kmeans(scale(whsAnnBdatNum),50,nstart =10)$tot.withinss/kmeans(scale(whsAnnBdatNum),50,nstart =10)$betweenss))

df
tt = table(df)
tt
```

When made the set.seed as constant as 100,the lower cluster with lower nstart gives the same ratios.  
But when use higher cluster with higher nstart, the ratios differed but stayed in close range.  

All the values are non-zero values in the dataframe.    

* Try using `silhouette` from the library `cluster` as another tool for assessing cluster strength for some of the clusters obtained here and describe the results

```{r}
plot(silhouette(pam(scale(whsAnnBdatNum),2)))
max(silhouette(pam(scale(whsAnnBdatNum),2)))
which.max(silhouette(pam(scale(whsAnnBdatNum),2)))

plot(silhouette(pam(scale(whsAnnBdatNum),4)))
max(silhouette(pam(scale(whsAnnBdatNum),4)))
which.max(silhouette(pam(scale(whsAnnBdatNum),4)))

plot(silhouette(pam(scale(whsAnnBdatNum),7)))
max(silhouette(pam(scale(whsAnnBdatNum),7)))
which.max(silhouette(pam(scale(whsAnnBdatNum),7)))

```

# Problem 3: Hierarchical clustering (20 points)

## Sub-problem 3a: hierachical clustering by different linkages (10 points)

Cluster country states in (scaled) world health statistics data using default (Euclidean) distance and "complete", "average", "single" and "ward" linkages in the call to `hclust`.  Plot each clustering hierarchy, describe the differences.  For comparison, plot results of clustering *untransformed* WHS data using default parameters (Euclidean distance, "complete" linkage) -- discuss the impact of the scaling on the outcome of hierarchical clustering.

```{r, fig.height=10, fig.width=10}
hc.complete=hclust(dist(scale(whsAnnBdatNum)), method ="complete")
hc.average=hclust(dist(scale(whsAnnBdatNum)), method ="average")
hc.single=hclust(dist(scale(whsAnnBdatNum)), method ="single")
hc.ward=hclust(dist(scale(whsAnnBdatNum)), method ="ward")

#par(mfrow =c(1,4))
plot(hc.complete,main="Complete Linkage")

plot(hc.average , main =" Average Linkage ")
plot(hc.single , main=" Single Linkage ")
plot(hc.ward , main=" Ward Linkage ")

hc.untrans=hclust(dist(whsAnnBdatNum), method ="complete")
plot(hc.untrans , main=" Complete Untransformed Linkage ")

```

Scaling gives more balanced, attractive clusters than the untransformed.  
India is always on the left because it is the pnly country thatlies in the positive while the rest are in the negative.  
All the plots gave 3 main clusters ecept the ward gave four.  

## Sub-problem 3b: compare k-means and hierarchical clustering (5 points)

Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and "complete" linkage.  Use function `table` to compare membership of these clusters to those produced by k-means clustering with four clusters in the Problem 2(c) above.  Discuss the results.

```{r}
cutree (hc.complete , 2)

table(k1$cluster ,cutree (hclust (dist(scale(whsAnnBdatNum))) ,4) )
table(k2$cluster ,cutree (hclust (dist(scale(whsAnnBdatNum))) ,4) )
table(k3$cluster ,cutree (hclust (dist(scale(whsAnnBdatNum))) ,4) )
```

## Sub-problem 3c: cluster variables by correlation (5 points)

Use (casted as distance) one-complement of Spearman correlation between *attributes* in world health statistics dataset to cluster *attributes* of WHS dataset.  E.g. `hclust(as.dist(1-cor(xyz,method="spearman")))` would cluster columns (as opposed to rows) in the matrix `xyz`.  Plot the results -- which variables tend to cluster together, why do you think that is?  Compare results obtained by this approach for scaled and untransformed WHS dataset?  How do they compare? What do you think is the explanation?


```{r}
#xyz=matrix (rnorm (30*3) , ncol =3)
plot(hclust (as.dist(1-cor(scale(whsAnnBdatNum),method="spearman"))), main=" Scaled dataset ", xlab="", sub ="")
```

```{r}
plot(hclust (as.dist(1-cor(whsAnnBdatNum,method="spearman"))), main=" Untransformed ", xlab="", sub ="")
```

They give the same plot with the same hierarchical clustering for the scaled and untransformed dataset.

### For *extra 4 points*

Use contingency tables to compare cluster memberships for several top clusters across different choices of linkage (e.g. "complete","ward","single") and distance (Euclidean, Manhattan, one-complement of correlation coefficient).  Discuss the results.

```{r}
#table(kmeans(scale(whsAnnBdatNum),2,nstart=20)$cluster,cutree (hclust (dist(scale(whsAnnBdatNum))) #,4) )
#table(hclust (as.dist(1-cor(scale(whsAnnBdatNum)))),hclust(dist(scale(whsAnnBdatNum),method ="complete")))


table(cutree (hclust (dist(scale(whsAnnBdatNum), method = "euclidean"), method ="complete") , 4),cutree (hclust (dist(scale(whsAnnBdatNum), method = "manhattan"), method ="ward") , 4))
```

```
