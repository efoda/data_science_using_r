---
title: "Egy Fouda-CSCI E-63C Week 8 Assignment"
output: html_document
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

In this assignment we will exercise some of the measures for evaluating "goodness of clustering" presented in the lecture this week on the clusters obtained for the World Health Statistics (WHS) dataset from week 6.  Please feel free to adapt/reuse code presented in lecture slides as necessary or implementations already available in R.  All problems in this assignment are expected to be performed on *scaled* WHS data -- if somewhere it does not mention it explicitly, please assume that it is scaled data that should be used. 

Lastly, as a dose of reality check: WHS is a dataset capturing variability of population health measures across more or less the entire diversity of societies in the world -- please be prepared to face the fact that resulting clustering structures are far from textbook perfect, may not be very clearly defined, etc.

## Note on quakes data (and *3 extra points per problem*) 

As you will notice, WHS dataset does not have the most striking cluster structure to it - at least as far as formal measurements of cluster strength that we are working with in this assignment are concerned (or the notion that there is well defined "optimal" number of clusters when split of observations into larger or smaller groups results in "worse" metrics). Not an uncommon situation for the data we have to work with at all.

As an opportunity to see the output of the code that you are using/developing for this assignment when applied to a dataset with more distinct substructure (and earn extra points by doing that)  for each of the five problems there are in this assignment (four required, one for extra points) once you generated required plots for WHS dataset, adding the same kinds of plots but for a standard R dataset "quakes" will be earning *3 extra points* for each problem.  So that if everything works perfectly this could add 15 extra points to the total to this assignment (5 problems including an extra point problem times 3 extra points each) so that along with the extra 5 points problem below, this assignment has potential of adding up to 20 extra points to your homework total.

Dataset "quakes" is routinely available in R upon log in - to "see" it, the following should just work without any further steps for a standard R installation:

```{r}
clr <- gray((quakes$depth-min(quakes$depth))/range(quakes$depth)%*%c(-1,1))
plot(quakes$lat,quakes$long,col=clr)
```
 
or, similarly, if you are a ggplot fan (in which case you will know to load ggplot2 library first):

```{r}
ggplot(quakes,aes(x=lat,y=long,colour=depth))+geom_point()
```
 
If you write your code with reusability in mind, applying it to "quakes" should be just a straightforward drop in replacement of WHS data frame with that of "quakes".  You will see that the subclasses of observations are so well defined in "quakes" that is almost boring in its own way.  Nothing is perfect in this world, but you should see more interesting behavior of CH index in this case, for example.

```{r WHS}
whsAnnBdatNum <- read.table("C:/Users/Engy/Downloads/Harvard/7th semester ISA/R/Week 8/whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")
scaledWHS<-scale(whsAnnBdatNum)
```

# Problem 1: within/between cluster variation and CH-index (15 points)

Present plots of (total) within and between cluster variance provided by K-means clustering on scaled WHS data for 2 through 20 clusters.  Choose large enough value of `nstart` for better stability of the results across multiple trials and evaluate stability of those results across several runs.  Discuss the results and whether the shape of the curves suggest specific number of clusters in the data.

#within following the lecture code
```{r}
w=numeric(20)

for ( k in 1:20 ) {
  kf=kmeans(scaledWHS,k)
  w[k] = kf$tot.withinss
}
plot(1:20,w,type="b",lwd=2,pch=19,xlab="K", ylab=expression("SS[within]"), main = "Within Cluster")
for ( i in 1:10 ) {
  wrnd = numeric()
  for ( k in 1:20 ) {
    krnd = kmeans(apply(scaledWHS,2,function(x)runif(length(x),min(x),max(x))),k)
    wrnd[k] = krnd$tot.withinss
  }
  points(wrnd,type="l",col="red")
}
```

As discussed in the lecture:
-Random sample of uniform distribution bounded by range of every attribute is commonly used alternative
-In this case total within cluster variance decreases faster for original data as compared to random sample  

#within adding the nstart and kmean from 2 to 20
```{r}
w=numeric(20)

for ( k in 2:20 ) {
  kf=kmeans(scaledWHS,k,nstart = 20)
  w[k] = kf$tot.withinss
}
plot(1:20,w,type="b",lwd=2,pch=19,xlab="K", ylab=expression("SS[within]"), main = "Within Cluster")
for ( i in 1:10 ) {
  wrnd = numeric()
  for ( k in 2:20 ) {
    krnd = kmeans(apply(scaledWHS,2,function(x)runif(length(x),min(x),max(x))),k,nstart = 20)
    wrnd[k] = krnd$tot.withinss
  }
  points(wrnd,type="l",col="red")
}
```



It gives a horrible graph and entirely different from that of the lecture  

#between
```{r}
btw=numeric(20)
for ( k in 1:20 ) {
kf=kmeans(scaledWHS,k)
btw[k] = kf$betweenss
}
plot(1:20,btw,type="b",lwd=2,pch=19,xlab="K", ylab=expression("SS[between]"), main = "Between Cluster")
for ( i in 1:10 ) {
btwrnd = numeric()
for ( k in 1:20 ) {
krnd = kmeans(apply(scaledWHS,2,function(x)runif(length(x),min(x),max(x))),k)
btwrnd[k] = krnd$betweenss
}
points(btwrnd,type="l",col="red")
}
```

For samples from uniform distribution between cluster sum of squares also increases rapidly for the first few clusters  

#CH-index, trying different values of n
```{r}
w=numeric(20)

for(k in 2:20){
kf=kmeans(scaledWHS,k,nstart=1)
w[k]=(kf$betweenss/(k-1))/(kf$tot.withinss/(length(kf$cluster)-k))
}
plot(2:20,w[-1],type="b",lwd=2,pch=19,xlab="K",ylab="CH index")

for(k in 2:20){
kf=kmeans(scaledWHS,k,nstart=5)
w[k]=(kf$betweenss/(k-1))/(kf$tot.withinss/(length(kf$cluster)-k))
}
plot(2:20,w[-1],type="b",lwd=2,pch=19,xlab="K",ylab="CH index")

for(k in 2:20){
kf=kmeans(scaledWHS,k,nstart=20)
w[k]=(kf$betweenss/(k-1))/(kf$tot.withinss/(length(kf$cluster)-k))
}
plot(2:20,w[-1],type="b",lwd=2,pch=19,xlab="K",ylab="CH index")


for(k in 2:20){
kf=kmeans(scaledWHS,k,nstart=50)
w[k]=(kf$betweenss/(k-1))/(kf$tot.withinss/(length(kf$cluster)-k))
}
plot(2:20,w[-1],type="b",lwd=2,pch=19,xlab="K",ylab="CH index")


for(k in 2:20){
kf=kmeans(scaledWHS,k,nstart=100)
w[k]=(kf$betweenss/(k-1))/(kf$tot.withinss/(length(kf$cluster)-k))
}
plot(2:20,w[-1],type="b",lwd=2,pch=19,xlab="K",ylab="CH index")


```


By trying different nstart values, starting from nstart =20 gives smooth plots.  

#CH-index (following the section code and changing the nstart to 20)
```{r}
chori=numeric(20)
for ( k in 2:20 ) {
  kf=kmeans(scaledWHS,k,nstart=20)
  chori[k] = (kf$betweenss/(k-1)) / (kf$tot.withinss/(nrow(scaledWHS)-k))
}
plot(2:20,chori[-1],type="b", lwd=2,pch=19,xlab="K", ylab="CH index",xlim=c(1,20),ylim=range(chori[-1])*c(1/2,1))

for ( i in 1:10 ) {
  chrnd = numeric()
  for ( k in 2:20 ) {
    krnd = kmeans(apply(scaledWHS,2,function(x)runif(length(x),min(x),max(x))),k,nstart=20)
    chrnd[k] = (krnd$betweenss/(k-1)) / (krnd$tot.withinss/(nrow(scaledWHS)-k))
  }
  points(2:20,chrnd[-1],type="l",col="red")
}
```

The shape of the curves suggests number of clusters in the data is equal to 3 where the elbow is.

As discussed in the lecture:
-CH-index achieves much higher values on original as compared to random data from uniform distribution with the same range  
-Value of K corresponding to the max. CH-index does not imply that this is the number of "real" clusters
  
#For Extra quakes-No scale dataset
```{r}

chori=numeric(20)
for ( k in 2:20 ) {
  kf=kmeans(quakes,k,nstart=10)
  chori[k] = (kf$betweenss/(k-1)) / (kf$tot.withinss/(nrow(quakes)-k))
}
plot(2:20,chori[-1],type="b", lwd=2,pch=19,xlab="K", ylab="CH index",xlim=c(1,20),ylim=range(chori[-1])*c(1/2,1))

for ( i in 1:10 ) {
  chrnd = numeric()
  for ( k in 2:20 ) {
    krnd = kmeans(apply(quakes,2,function(x)runif(length(x),min(x),max(x))),k,nstart=10)
    chrnd[k] = (krnd$betweenss/(k-1)) / (krnd$tot.withinss/(nrow(quakes)-k))
  }
  points(2:20,chrnd[-1],type="l",col="red")
}
```
#For Extra quakes-scaled dataset
```{r}
Squakes<-scale(quakes)
chori=numeric(20)
for ( k in 1:20 ) {
  kf=kmeans(Squakes,k,nstart=20)
  chori[k] = (kf$betweenss/(k-1)) / (kf$tot.withinss/(nrow(Squakes)-k))
}
plot(2:20,chori[-1],type="b", lwd=2,pch=19,xlab="K", ylab="CH index",xlim=c(1,20),ylim=range(chori[-1])*c(1/2,1))

for ( i in 1:10 ) {
  chrnd = numeric()
  for ( k in 1:20 ) {
    krnd = kmeans(apply(Squakes,2,function(x)runif(length(x),min(x),max(x))),k,nstart=20)
    chrnd[k] = (krnd$betweenss/(k-1)) / (krnd$tot.withinss/(nrow(Squakes)-k))
  }
  points(2:20,chrnd[-1],type="l",col="red")
}
```

The scaled quakes gives better results; hence, will stick to it in the rest of the assignment
```{r}
quakes<-scale(quakes)
```

# Problem 2: gap statistics (15 points)

Using code provided in the lecture slides for calculating gap statistics or one of its implementations available in R (e.g. `clusGap` from library `cluster`) compute and plot gap statistics for K-means clustering of scaled WHS data for 2 through 20 clusters.  Discuss whether it indicates presence of clearly defined cluster structure in this data.

Disclaimer: I followed the steps in this link
https://www.rdocumentation.org/packages/cluster/versions/2.0.5/topics/clusGap

```{r}
# gsP.Z <- clusGap(scaledWHS, FUN = pam, K.max = 20, B = 200)
# plot(gsP.Z)

cgScaled <- clusGap(scaledWHS,kmeans,20,d.power=2)
plot(cgScaled)

head(cgScaled$Tab)
maxSE(cgScaled$Tab[,"gap"],cgScaled$Tab[,"SE.sim"])
```

As Andry answered me in piazza:
"The "optimal number of clusters" is determined by shape/maxima of the gap statistics as a function of K, not by the scale factor of the curve."    
From the lecture:  
Yes, the plot indicates presence of clearly defined cluster structure in this data because if there is no cluster structure we expect the clusters to be "bad" and wide and if there are true clusters in the data, we expect them to be "tighter" than those seen" uniform random.  

#For extra quakes dataset

```{r}

cgScaled <- clusGap(quakes,kmeans,20,d.power=2)
plot(cgScaled)

head(cgScaled$Tab)
maxSE(cgScaled$Tab[,"gap"],cgScaled$Tab[,"SE.sim"])
```


# Problem 3: stability of hierarchical clustering (15 points)

For top 2, 3 and 4 clusters (as obtained by `cutree` at corresponding levels of `k`) found by Ward method in `hclust` and by K-means when applied to the scaled WHS data compare cluster memberships between these two methods and describe their concordance.  This problem is similar to the one in 6th week assignment, but this time it is *required* to: 1) use two dimensional contingency tables implemented by `table` to compare membership between two assignments of observations to clusters, and 2) programmatically re-order rows and columns in the `table` outcome in the increasing order of observations shared between two clusters (please see examples in lecture slides).

```{r}
# matrix.sort <- function(m) {
# if (nrow(m) != ncol(m)) { stop("Not diagonal") } 
# if(is.null(rownames(m))) { rownames(m) = 1:nrow(matrix)}
# row.max = apply(m,1,which.max) 
# if(any(table(row.max) != 1)) { col.max = apply(m,2,which.max) 
# if ( any(table(col.max)!=1) ) { warning("Ties cannot be resolved") 
# }
# return(m[,order(col.max)])
# }
# m[order(row.max),]
# }
matrix.sort <- function(m) {
    require(clue)
    p = solve_LSAP(m,maximum=T)
     # now we can rearrange and return m
     m[, p]
}
```


```{r}
kmTmp <- kmeans(scaledWHS,4,nstart=100)
matrix.sort(table(kmTmp$cluster,cutree(hclust(dist(scaledWHS),method="ward.D"),k=4)))
```

#For extra quakes dataset

```{r}
kmTmp <- kmeans(quakes,4,nstart=100)
matrix.sort(table(kmTmp$cluster,cutree(hclust(dist(quakes),method="ward.D"),k=4)))
```


## For *extra* 5 points: between/within variance in hierarchical clusters

Using functions `between` and `within` provided in the lecture slides calculate between and (total) within cluster variances for top 2 through 20 clusters defined by Ward's hierarchical clustering when applied to scaled WHS data.  Plot the results.  Compare their behavior to that of the same statistics when obtained for K-means clustering above.

```{r}
within=function(d,clust) { w=numeric(length(unique(clust))) 
for ( i in sort(unique(clust)) ) { 
  members = d[clust==i,,drop=F] 
  centroid = apply(members,2,mean) 
  members.diff = sweep(members,2,centroid) 
  w[i] = sum(members.diff^2)
}
return(w)
}
```

```{r}
between=function(d,clust) { 
  b=0
total.mean = apply(d,2,mean) 
for ( i in sort(unique(clust)) ) { 
  members = d[clust==i,,drop=F] 
  centroid = apply(members,2,mean) 
  b = b + nrow(members)* sum( (centroid-total.mean)^2 )
}
return(b)
}
```

```{r}
dd.1=dist(scaledWHS) 
hw.1=hclust(dd.1,method="ward.D2")
w.tot=numeric(9) 
btw=numeric(9) 
for ( k in 2:20 ) { 
  clust = cutree(hw.1,k=k) 
  w = within(scaledWHS,clust)
w.tot[k-1]=sum(w)
btw[k-1] = between(scaledWHS,clust)
}
plot(2:20,w.tot,pch=19,type="b") 
plot(2:20,btw,pch=19,type="b") 
plot(2:20,(btw/(1:19))/(w.tot/(nrow(scaledWHS)-2:20)),pch=19,type="b")
```

the kmeans plot shows clearly the elbow and indicates the nuber of clusters, while here in the hierarchial the curves are so smooth  
As mentioned in the lecture:
The clear cluster structure is missing from the hierarchical clustering result  

#For extra quakes dataset

```{r}
dd.2=dist(quakes) 
hw.2=hclust(dd.1,method="ward.D2")
w.tot=numeric(9) 
btw=numeric(9) 
for ( k in 2:10 ) { 
  clust = cutree(hw.1,k=k) 
  w = within(quakes,clust)
w.tot[k-1]=sum(w)
btw[k-1] = between(quakes,clust)
}
plot(2:10,w.tot,pch=19,type="b") 
plot(2:10,btw,pch=19,type="b") 
plot(2:10,(btw/(1:9))/(w.tot/(nrow(quakes)-2:10)),pch=19,type="b")
```

# Problem 4: Brute force randomization in hierarchical clustering (15 points)

Compare distribution of the heights of the clusters defined by `hclust` with Ward's clustering of Euclidean distance between countries in scaled WHS dataset and those obtained by applying the same approach to the distances calculated on randomly permuted WHS dataset as illustrated in the lecture slides.  Discuss whether results of such brute force randomization are supportive of presence of unusually close or distant sets of observations within WHS data.

```{r}
ori.heights = hw.1$height 
rnd.heights = numeric() 
for ( i.sim in 1:100 ) { 
  data.rnd <-apply(scaledWHS,2,sample)
hw.rnd=hclust(dist(data.rnd),method="ward.D2") 
rnd.heights <- c(rnd.heights,hw.rnd$height)
}
plot(ori.heights,rank(ori.heights)/length(ori.heights), col="red",xlab="height",ylab="F(height)",pch=19)
points(rnd.heights,rank(rnd.heights)/length(rnd.heights), col="blue")

```

yes, the results of such brute force randomization are supportive of presence of unusually close or distant sets of observations within WHS data.
Every time the graph is changed because of the randomization. For what I see at the moment, the blue curve that represents the randomization ranges between 2 to about 25.  
The real data has a couple of points above 25, and one above 60, so there is a hope that the reaal data has subclusters more than the randomized data.  
About 3 top level clusters might be real.  

#For extra quakes dataset

```{r}
ori.heights = hw.2$height 
rnd.heights = numeric() 
for ( i.sim in 1:100 ) { 
  data.rnd <-apply(quakes,2,sample)
hw.rnd=hclust(dist(data.rnd),method="ward.D2") 
rnd.heights <- c(rnd.heights,hw.rnd$height)
}
plot(ori.heights,rank(ori.heights)/length(ori.heights), col="red",xlab="height",ylab="F(height)",pch=19)
points(rnd.heights,rank(rnd.heights)/length(rnd.heights), col="blue")

```



